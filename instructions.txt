This will be a comparison between federated learning and centralized machine learning over an ML model being trained with a dataset.
Dataset: https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones
This dataset comes from different sensors and gyroscopes from Smartphones worn by 30 different subjects who performed different activities.
The activities in turn became the different labels, 6 in total of each row of data generated by the Smartphones.
A neural network will be trained and tested using this data for the purposes of classification.
The NN will be trained both in a centralized manner and with a Federated Learnign simulation, both using different numbers of random subjects.
Both will be tested using randomly split data from all the subjects in a central manner as to make the comparison as impartial as possible


In order to run the FL simulation the ETL pipeline must be run in order. They will be listed in order that must be run, along with an explanation of what they do

First comes Extraction.
  -Extract.py is the first file that must be run, it will download the dataset
  -Extract_from_zip.py is the next file, it will extract the dataset from the downloaded file
Next up is Transformation
  -initial_file_treatment.py is the first script from the transformation phase, it will remove some extra spaces, change the encoding on some of the files 
   and drop the treated files off in a new folder.
   -unify_data.py is next, this script will take the files from the aforemention new folder and unite all the data into a single file. The data is originally 
   divided six ways. First there is the train-test division typical in any ML setting, but we are interested in randomizing the train-test split later down the line.
   Both train and test are also subdivided into 3 files each, thus making a total of six files, those files are features, labels and subjects. The last one is which
   subject generated each row of data.
   -dummy_columns_and_header.py is the script that follow unify_data.py. It will separate the label column into six different dummy columns, this step is important for
   later classification. A header will also be added since is missing from the files
   -select_features.py, there are around 560 features, using all of them would be time consuming and might lead to overfitting, so using tree classifiers the most 
   important features from each class are selected. And the data file is then filtered and saved.
   -data separation by subject.py This will separate the data into 30 different files based on the subject that generated the data.
Not part of the ETL pipeline but must be run before the Loading phase
  -Client_Creation.py, Takes the original client.py file (client.py doesn't have to be manually run) and creates copies, each one for a different subject, so as to simulate
  Federated Learning.
Finally comes the Loading phase   
   -FL and Centralized ML test.py, this go over the different specified numbers of clients (Specified in line 99, changes recomended according to available RAM and time),
   and for each number run 10 FL and centralized trainings and testings with the data and record the results.
   -lineplot_seaborn.py, takes the results from the previous script and makes Data Visualizations using seaborn.
   
   
   
   
 Note:
 Since "FL and Centralized ML test.py" can take a long time, interrupting it and continuing later might be necesary, that script isn't meant to do that and every time is
 is run it resets the results file. But "FL and Centralized ML test.py" allows to continue where the experiments where left off. But it requires to be specified the remaing
 numbers of subjects to be used and the number of times the experiment is to be run for every number of subjects in lines 89 and 90 respectively.
   
   
   
   
   
   
   
